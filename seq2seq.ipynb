{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Playground\\bots\\trading\\pocket-option-bot\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from trading_tools import data_frame\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSET = \"AUDCAD_otc\" \n",
    "# ASSET = \"EURUSD_otc\" \n",
    "df = data_frame.load_csv(f\"actives/ACTIVO-{ASSET}-0005s.csv\")\n",
    "# df = data_frame.load_csv(f\"actives/ACTIVO-{ASSET}-0001s.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_conditions_vectorized(df, window_size):\n",
    "    labels = np.zeros(len(df))\n",
    "    \n",
    "    # Create shifted arrays for high, low, and close\n",
    "    high_shifted = np.array([df['high'].shift(-i).values for i in range(window_size)])\n",
    "    low_shifted = np.array([df['low'].shift(-i).values for i in range(window_size)])\n",
    "    close_shifted = df['close'].shift(-window_size).values\n",
    "\n",
    "    # Initialize condition arrays\n",
    "    condition_high_down = np.all(high_shifted[1:] < high_shifted[:-1], axis=0)\n",
    "    condition_low_down = np.all(low_shifted[1:] < low_shifted[:-1], axis=0)\n",
    "    target_condition_down = close_shifted < df['close'].values\n",
    "    \n",
    "    condition_high_up = np.all(high_shifted[1:] > high_shifted[:-1], axis=0)\n",
    "    condition_low_up = np.all(low_shifted[1:] > low_shifted[:-1], axis=0)\n",
    "    target_condition_up = close_shifted > df['close'].values\n",
    "    \n",
    "    # Apply conditions\n",
    "    conditions_down = condition_high_down & condition_low_down & target_condition_down\n",
    "    conditions_up = condition_high_up & condition_low_up & target_condition_up\n",
    "    \n",
    "    # Adjust indices to ensure correct length\n",
    "    valid_conditions = conditions_down[:-window_size] | conditions_up[:-window_size]\n",
    "    labels[:len(valid_conditions)] = valid_conditions.astype(int)\n",
    "    labels[len(valid_conditions):] = np.nan\n",
    "    \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for check_conditions_vectorized: 0.12 seconds\n",
      "Conteo de valores en la columna seq_quality_6: seq_quality_6\n",
      "0.0    878644\n",
      "1.0     31072\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "window_size = 6\n",
    "QUALITY_LABEL = f'seq_quality_{window_size}'\n",
    "\n",
    "# Measure execution time for the vectorized method\n",
    "start_time = time.time()\n",
    "SEQ_LABEL = f'seq_quality_{window_size}'\n",
    "df[SEQ_LABEL] = check_conditions_vectorized(df, window_size)\n",
    "time_check_conditions_vectorized = time.time() - start_time\n",
    "print(f\"Time for check_conditions_vectorized: {time_check_conditions_vectorized:.2f} seconds\")\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "count_values = df[SEQ_LABEL].value_counts()\n",
    "\n",
    "print(f\"Conteo de valores en la columna {SEQ_LABEL}: {count_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62144\n",
      "Total zeros: 878644, Total ones: 31072\n",
      "Training data size: 43500, Training target size: 43500\n",
      "Validation data size: 12435, Validation target size: 12435\n",
      "Testing data size: 6209, Testing target size: 6209\n",
      "Class balance in training set: 21804 zeros and 21696 ones\n"
     ]
    }
   ],
   "source": [
    "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
    "num_features = df.shape[1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_columns = df.drop(columns=[SEQ_LABEL]).columns.copy()\n",
    "\n",
    "# Split the data into training and temporary sets first\n",
    "X_temp, y_temp = df.drop(columns=[SEQ_LABEL]).values, df[SEQ_LABEL].values\n",
    "\n",
    "# Identify the indices of each class\n",
    "zero_indices = [i for i, y in enumerate(y_temp) if y == 0]\n",
    "one_indices = [i for i, y in enumerate(y_temp) if y == 1]\n",
    "\n",
    "# Determine the smaller class size\n",
    "min_class_size = min(len(zero_indices), len(one_indices))\n",
    "\n",
    "# Randomly sample from each class to ensure balance\n",
    "np.random.seed(42)\n",
    "zero_sample = np.random.choice(zero_indices, min_class_size, replace=False)\n",
    "one_sample = np.random.choice(one_indices, min_class_size, replace=False)\n",
    "\n",
    "# Combine the sampled indices\n",
    "balanced_indices = np.concatenate([zero_sample, one_sample])\n",
    "\n",
    "# Create balanced datasets\n",
    "X_balanced = [X_temp[i] for i in balanced_indices]\n",
    "y_balanced = [y_temp[i] for i in balanced_indices]\n",
    "print(len(balanced_indices))\n",
    "# Split the data into training and temporary sets first\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Split the temporary set into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split( X_temp, y_temp, test_size=0.333, random_state=16, shuffle=True)\n",
    "# Verify the balance\n",
    "print(f\"Total zeros: {len(zero_indices)}, Total ones: {len(one_indices)}\")\n",
    "print(f\"Training data size: {len(X_train)}, Training target size: {len(y_train)}\")\n",
    "print(f\"Validation data size: {len(X_val)}, Validation target size: {len(y_val)}\")\n",
    "print(f\"Testing data size: {len(X_test)}, Testing target size: {len(y_test)}\")\n",
    "print(f\"Class balance in training set: {np.sum(np.array(y_train) == 0)} zeros and {np.sum(np.array(y_train) == 1)} ones\")\n",
    "\n",
    "@staticmethod\n",
    "def create_df(features, targets, feature_labels, target_label):\n",
    "    out_df = pd.DataFrame(data=features, columns=feature_labels)\n",
    "    out_df[target_label] = targets\n",
    "    return out_df\n",
    "    \n",
    "train_df = create_df(X_train, y_train, features_columns, SEQ_LABEL)\n",
    "val_df = create_df(X_val, y_val, features_columns, SEQ_LABEL)\n",
    "test_df = create_df(X_test, y_test, features_columns, SEQ_LABEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_columns, features_columns,\n",
    "                 train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                  batch_size=32):\n",
    "        # Almacenar los datos crudos\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        self.column_indices = {name: i for i, name in enumerate(train_df.columns)}\n",
    "        \n",
    "        self.label_columns = label_columns     \n",
    "           \n",
    "        self.features_columns = features_columns\n",
    "        \n",
    "\n",
    "        # Calcular los par√°metros de la ventana\n",
    "        self.input_width = input_width\n",
    "\n",
    "        self.total_window_size = input_width \n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.normalize_inputs = False\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {min(self.input_indices)} - {max(self.input_indices)}',\n",
    "            f'Label column name(s): {self.label_columns}',\n",
    "            f'Feature column name(s): {self.features_columns}'\n",
    "        ])\n",
    "    \n",
    "    @tf.function\n",
    "    def normalize(self, inputs):\n",
    "        def normalize_sequence(sequence):\n",
    "            rolling_mean = tf.reduce_mean(sequence, axis=0, keepdims=True)\n",
    "            return (sequence - rolling_mean) / rolling_mean\n",
    "\n",
    "        return tf.map_fn(normalize_sequence, inputs, fn_output_signature=tf.float32)\n",
    "\n",
    "    def split_window(self, features):\n",
    "        data = features[:, self.input_slice, :]\n",
    "        inputs = tf.stack([data[:, :, self.column_indices[name]] for name in self.features_columns], axis=-1)\n",
    "        labels = tf.stack([data[:, -1, self.column_indices[name]] for name in self.label_columns], axis=-1)\n",
    "\n",
    "        # if self.normalize_inputs:\n",
    "        #     inputs = self.normalize(tf.cast(inputs, tf.float32))  # Convertir inputs a float32\n",
    "\n",
    "        inputs.set_shape([None, self.input_width, len(self.features_columns)])\n",
    "        labels.set_shape([None, len(self.label_columns)])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = data.to_numpy()\n",
    "        ds = keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "        ds = ds.map(self.split_window)\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No se encontr√≥ un lote de ejemplo, as√≠ que obt√©n uno del dataset de entrenamiento\n",
    "            result = next(iter(self.train))\n",
    "            # Y gu√°rdalo en cach√© para la pr√≥xima vez\n",
    "            self._example = result\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Total window size: 120\n",
       "Input indices: 0 - 119\n",
       "Label column name(s): ['seq_quality_6']\n",
       "Feature column name(s): ['open', 'high', 'low', 'close']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_columns=[SEQ_LABEL]\n",
    "# features_columns=['open', \"close\"]\n",
    "features_columns= [col for col in df.columns if col not in label_columns]\n",
    "num_labels = len(label_columns)\n",
    "num_features = len(features_columns)\n",
    "input_width = 120\n",
    "\n",
    "window = WindowGenerator( input_width=input_width, label_columns= label_columns , features_columns=features_columns, batch_size=2048)\n",
    "\n",
    "window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "# Number of hidden neuros in each layer of the encoder and decoder\n",
    "layers = [35, 35] \n",
    "\n",
    "learning_rate = 0.01\n",
    "decay = 0 # Learning rate decay\n",
    "\n",
    "# Other possible optimiser \"sgd\" (Stochastic Gradient Descent)\n",
    "optimiser = keras.optimizers.legacy.Adam(learning_rate=learning_rate, decay=decay) \n",
    "\n",
    "# The dimensionality of the input at each time step. In this case a 1D signal.\n",
    "num_input_features = num_features \n",
    "# The dimensionality of the output at each time step. In this case a 1D signal.\n",
    "num_output_features = num_labels \n",
    "# There is no reason for the input sequence to be of same dimension as the ouput sequence.\n",
    "# For instance, using 3 input signals: consumer confidence, inflation and house prices to predict the future house prices.\n",
    "\n",
    "# Other loss functions are possible, see Keras documentation.\n",
    "loss = \"mse\" \n",
    "\n",
    "# Regularisation isn't really needed for this application\n",
    "lambda_regulariser = 0.000001 # Will not be used if regulariser is None\n",
    "regulariser = None # Possible regulariser: keras.regularizers.l2(lambda_regulariser)\n",
    "\n",
    "# batch_size * steps_per_epoch = total number of training examples\n",
    "batch_size = 2048\n",
    "steps_per_epoch = 200\n",
    "epochs = 15\n",
    "\n",
    "input_sequence_length = input_width # Length of the sequence used by the encoder\n",
    "target_sequence_length = window_size # Length of the sequence predicted by the decoder\n",
    "num_steps_to_predict = 20 # Length to use when testing the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=(None, num_input_features))\n",
    "\n",
    "# Create a list of RNN Cells, these are then concatenated into a single layer\n",
    "# with the RNN layer.\n",
    "encoder_cells = []\n",
    "for hidden_neurons in layers:\n",
    "    encoder_cells.append(keras.layers.GRUCell(hidden_neurons,\n",
    "                                              kernel_regularizer=regulariser,\n",
    "                                              recurrent_regularizer=regulariser,\n",
    "                                              bias_regularizer=regulariser))\n",
    "\n",
    "encoder = keras.layers.RNN(encoder_cells, return_state=True)\n",
    "\n",
    "encoder_outputs_and_states = encoder(encoder_inputs)\n",
    "\n",
    "# Discard encoder outputs and only keep the states.\n",
    "# The outputs are of no interest to us, the encoder's\n",
    "# job is to create a state describing the input sequence.\n",
    "encoder_states = encoder_outputs_and_states[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decoder input will be set to zero (see random_sine function of the utils module).\n",
    "# Do not worry about the input size being 1, I will explain that in the next cell.\n",
    "decoder_inputs = keras.layers.Input(shape=(None, 1))\n",
    "\n",
    "decoder_cells = []\n",
    "for hidden_neurons in layers:\n",
    "    decoder_cells.append(keras.layers.GRUCell(hidden_neurons,\n",
    "                                              kernel_regularizer=regulariser,\n",
    "                                              recurrent_regularizer=regulariser,\n",
    "                                              bias_regularizer=regulariser))\n",
    "\n",
    "decoder = keras.layers.RNN(decoder_cells, return_sequences=True, return_state=True)\n",
    "\n",
    "# Set the initial state of the decoder to be the ouput state of the encoder.\n",
    "# This is the fundamental part of the encoder-decoder.\n",
    "decoder_outputs_and_states = decoder(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "# Only select the output of the decoder (not the states)\n",
    "decoder_outputs = decoder_outputs_and_states[0]\n",
    "\n",
    "# Apply a dense layer with linear activation to set output to correct dimension\n",
    "# and scale (tanh is default activation for GRU in Keras, our output sine function can be larger then 1)\n",
    "decoder_dense = keras.layers.Dense(num_output_features,\n",
    "                                   activation='linear',\n",
    "                                   kernel_regularizer=regulariser,\n",
    "                                   bias_regularizer=regulariser)\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model using the functional API provided by Keras.\n",
    "# The functional API is great, it gives an amazing amount of freedom in architecture of your NN.\n",
    "# A read worth your time: https://keras.io/getting-started/functional-api-guide/ \n",
    "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "model.compile(optimizer=optimiser, loss=loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
