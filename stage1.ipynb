{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Playground\\bots\\trading\\pocket-option-bot\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from trading_tools import data_frame\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSET = \"AUDCAD_otc\" \n",
    "# ASSET = \"EURUSD_otc\" \n",
    "df = data_frame.load_csv(f\"actives/ACTIVO-{ASSET}-0005s.csv\")\n",
    "# df = data_frame.load_csv(f\"actives/ACTIVO-{ASSET}-0001s.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAGE 1\n",
    "##### Identifing valid sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_conditions(df, window_size):\n",
    "    \"\"\"\n",
    "    ### Conditions are:\n",
    "    It is a valid sequence if all the candles after the end of the sequence have high and lows higher or lower than those in the previous candle\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    count = len(df) - window_size\n",
    "\n",
    "    for i in tqdm(range(count), desc=\"Processing\"):\n",
    "        close0 = df['close'].iloc[i]\n",
    "        close1 = df['close'].iloc[i+window_size]\n",
    "        \n",
    "        window = df.iloc[i:i+window_size]\n",
    "        condition_high_down = all(window['high'].iloc[j] < window['high'].iloc[j-1] for j in range(1, window_size))\n",
    "        condition_low_down = all(window['low'].iloc[j] < window['low'].iloc[j-1] for j in range(1, window_size))\n",
    "        target_condition_down = close1 < close0\n",
    "        \n",
    "        condition_high_up = all(window['high'].iloc[j] > window['high'].iloc[j-1] for j in range(1, window_size))\n",
    "        condition_low_up = all(window['low'].iloc[j] > window['low'].iloc[j-1] for j in range(1, window_size))\n",
    "        target_condition_up = close1 > close0\n",
    "        \n",
    "        if (condition_high_down and condition_low_down and target_condition_down) or (condition_high_up and condition_low_up and target_condition_up):\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    \n",
    "    # Add NaNs for the tail of the DataFrame that doesn't have enough data to form a full window\n",
    "    labels.extend([None] * window_size)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_conditions_vectorized(df, window_size):\n",
    "    labels = np.zeros(len(df))\n",
    "    \n",
    "    # Create shifted arrays for high, low, and close\n",
    "    high_shifted = np.array([df['high'].shift(-i).values for i in range(window_size)])\n",
    "    low_shifted = np.array([df['low'].shift(-i).values for i in range(window_size)])\n",
    "    close_shifted = df['close'].shift(-window_size).values\n",
    "\n",
    "    # Initialize condition arrays\n",
    "    condition_high_down = np.all(high_shifted[1:] < high_shifted[:-1], axis=0)\n",
    "    condition_low_down = np.all(low_shifted[1:] < low_shifted[:-1], axis=0)\n",
    "    target_condition_down = close_shifted < df['close'].values\n",
    "    \n",
    "    condition_high_up = np.all(high_shifted[1:] > high_shifted[:-1], axis=0)\n",
    "    condition_low_up = np.all(low_shifted[1:] > low_shifted[:-1], axis=0)\n",
    "    target_condition_up = close_shifted > df['close'].values\n",
    "    \n",
    "    # Apply conditions\n",
    "    conditions_down = condition_high_down & condition_low_down & target_condition_down\n",
    "    conditions_up = condition_high_up & condition_low_up & target_condition_up\n",
    "    \n",
    "    # Adjust indices to ensure correct length\n",
    "    valid_conditions = conditions_down[:-window_size] | conditions_up[:-window_size]\n",
    "    labels[:len(valid_conditions)] = valid_conditions.astype(int)\n",
    "    labels[len(valid_conditions):] = np.nan\n",
    "    \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for check_conditions_vectorized: 0.12 seconds\n",
      "Conteo de valores en la columna seq_quality_5: seq_quality_5\n",
      "0.0    853723\n",
      "1.0     67151\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "window_size = 5\n",
    "QUALITY_LABEL = f'seq_quality_{window_size}'\n",
    "\n",
    "# Measure execution time for the vectorized method\n",
    "start_time = time.time()\n",
    "SEQ_LABEL = f'seq_quality_{window_size}'\n",
    "df[SEQ_LABEL] = check_conditions_vectorized(df, window_size)\n",
    "time_check_conditions_vectorized = time.time() - start_time\n",
    "print(f\"Time for check_conditions_vectorized: {time_check_conditions_vectorized:.2f} seconds\")\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "count_values = df[SEQ_LABEL].value_counts()\n",
    "\n",
    "print(f\"Conteo de valores en la columna {SEQ_LABEL}: {count_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134302\n",
      "Total zeros: 853723, Total ones: 67151\n",
      "Training data size: 94011, Training target size: 94011\n",
      "Validation data size: 26874, Validation target size: 26874\n",
      "Testing data size: 13417, Testing target size: 13417\n",
      "Class balance in training set: 46905 zeros and 47106 ones\n"
     ]
    }
   ],
   "source": [
    "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
    "num_features = df.shape[1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_columns = df.drop(columns=[SEQ_LABEL]).columns.copy()\n",
    "\n",
    "# Split the data into training and temporary sets first\n",
    "X_temp, y_temp = df.drop(columns=[SEQ_LABEL]).values, df[SEQ_LABEL].values\n",
    "\n",
    "# Identify the indices of each class\n",
    "zero_indices = [i for i, y in enumerate(y_temp) if y == 0]\n",
    "one_indices = [i for i, y in enumerate(y_temp) if y == 1]\n",
    "\n",
    "# Determine the smaller class size\n",
    "min_class_size = min(len(zero_indices), len(one_indices))\n",
    "\n",
    "# Randomly sample from each class to ensure balance\n",
    "np.random.seed(42)\n",
    "zero_sample = np.random.choice(zero_indices, min_class_size, replace=False)\n",
    "one_sample = np.random.choice(one_indices, min_class_size, replace=False)\n",
    "\n",
    "# Combine the sampled indices\n",
    "balanced_indices = np.concatenate([zero_sample, one_sample])\n",
    "\n",
    "# Create balanced datasets\n",
    "X_balanced = [X_temp[i] for i in balanced_indices]\n",
    "y_balanced = [y_temp[i] for i in balanced_indices]\n",
    "print(len(balanced_indices))\n",
    "# Split the data into training and temporary sets first\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Split the temporary set into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split( X_temp, y_temp, test_size=0.333, random_state=16, shuffle=True)\n",
    "# Verify the balance\n",
    "print(f\"Total zeros: {len(zero_indices)}, Total ones: {len(one_indices)}\")\n",
    "print(f\"Training data size: {len(X_train)}, Training target size: {len(y_train)}\")\n",
    "print(f\"Validation data size: {len(X_val)}, Validation target size: {len(y_val)}\")\n",
    "print(f\"Testing data size: {len(X_test)}, Testing target size: {len(y_test)}\")\n",
    "print(f\"Class balance in training set: {np.sum(np.array(y_train) == 0)} zeros and {np.sum(np.array(y_train) == 1)} ones\")\n",
    "\n",
    "@staticmethod\n",
    "def create_df(features, targets, feature_labels, target_label):\n",
    "    out_df = pd.DataFrame(data=features, columns=feature_labels)\n",
    "    out_df[target_label] = targets\n",
    "    return out_df\n",
    "    \n",
    "train_df = create_df(X_train, y_train, features_columns, SEQ_LABEL)\n",
    "val_df = create_df(X_val, y_val, features_columns, SEQ_LABEL)\n",
    "test_df = create_df(X_test, y_test, features_columns, SEQ_LABEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage1WindowGenerator():\n",
    "    def __init__(self, input_width, label_columns, features_columns,\n",
    "                 train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                  batch_size=32):\n",
    "        # Almacenar los datos crudos\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        self.column_indices = {name: i for i, name in enumerate(train_df.columns)}\n",
    "        \n",
    "        self.label_columns = label_columns     \n",
    "           \n",
    "        self.features_columns = features_columns\n",
    "        \n",
    "\n",
    "        # Calcular los par√°metros de la ventana\n",
    "        self.input_width = input_width\n",
    "\n",
    "        self.total_window_size = input_width \n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.normalize_inputs = False\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {min(self.input_indices)} - {max(self.input_indices)}',\n",
    "            f'Label column name(s): {self.label_columns}',\n",
    "            f'Feature column name(s): {self.features_columns}'\n",
    "        ])\n",
    "    \n",
    "    @tf.function\n",
    "    def normalize(self, inputs):\n",
    "        def normalize_sequence(sequence):\n",
    "            rolling_mean = tf.reduce_mean(sequence, axis=0, keepdims=True)\n",
    "            return (sequence - rolling_mean) / rolling_mean\n",
    "\n",
    "        return tf.map_fn(normalize_sequence, inputs, fn_output_signature=tf.float32)\n",
    "\n",
    "    def split_window(self, features):\n",
    "        data = features[:, self.input_slice, :]\n",
    "        inputs = tf.stack([data[:, :, self.column_indices[name]] for name in self.features_columns], axis=-1)\n",
    "        labels = tf.stack([data[:, -1, self.column_indices[name]] for name in self.label_columns], axis=-1)\n",
    "\n",
    "        # if self.normalize_inputs:\n",
    "        #     inputs = self.normalize(tf.cast(inputs, tf.float32))  # Convertir inputs a float32\n",
    "\n",
    "        inputs.set_shape([None, self.input_width, len(self.features_columns)])\n",
    "        labels.set_shape([None, len(self.label_columns)])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = data.to_numpy()\n",
    "        ds = keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "        ds = ds.map(self.split_window)\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No se encontr√≥ un lote de ejemplo, as√≠ que obt√©n uno del dataset de entrenamiento\n",
    "            result = next(iter(self.train))\n",
    "            # Y gu√°rdalo en cach√© para la pr√≥xima vez\n",
    "            self._example = result\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Total window size: 30\n",
       "Input indices: 0 - 29\n",
       "Label column name(s): ['seq_quality_5']\n",
       "Feature column name(s): ['open', 'high', 'low', 'close']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_columns=[SEQ_LABEL]\n",
    "# features_columns=['open', \"close\"]\n",
    "features_columns= [col for col in df.columns if col not in label_columns]\n",
    "num_labels = len(label_columns)\n",
    "num_features = len(features_columns)\n",
    "input_width = 30\n",
    "\n",
    "window = Stage1WindowGenerator( input_width=input_width, label_columns= label_columns , features_columns=features_columns, batch_size=2048)\n",
    "\n",
    "window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv1D(32, 3, activation='relu'),\n",
    "    keras.layers.LSTM(64, return_sequences=True),\n",
    "    keras.layers.LSTM(32, return_sequences=False),\n",
    "    # keras.layers.Dense(128, activation='relu'),\n",
    "    # keras.layers.Dense(128, activation='relu'),\n",
    "    # keras.layers.Dropout(0.2),\n",
    "    # keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    # keras.layers.TimeDistributed( keras.layers.Dense(1, activation='sigmoid'))\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "def custom_binary_loss(y_true, y_pred):\n",
    "    # Force predictions to be strictly 0 or 1\n",
    "    y_pred_binary = keras.backend.round(y_pred)\n",
    "\n",
    "    # Binary cross-entropy loss\n",
    "    bce_loss = keras.backend.binary_crossentropy(y_true, y_pred_binary)\n",
    "\n",
    "    # Penalty term to push y_pred closer to 0 or 1\n",
    "    penalty = keras.backend.square(y_pred - y_pred_binary)\n",
    "\n",
    "    # Total loss\n",
    "    return keras.backend.mean(bce_loss + penalty)\n",
    "\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=custom_binary_loss, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From d:\\Playground\\bots\\trading\\pocket-option-bot\\.venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Playground\\bots\\trading\\pocket-option-bot\\.venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "46/46 [==============================] - 19s 213ms/step - loss: 0.6934 - accuracy: 0.4983 - val_loss: 0.6931 - val_accuracy: 0.5036 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 8s 171ms/step - loss: 0.6932 - accuracy: 0.5011 - val_loss: 0.6932 - val_accuracy: 0.4964 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 8s 172ms/step - loss: 0.6932 - accuracy: 0.5021 - val_loss: 0.6932 - val_accuracy: 0.4964 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Create directories for saving model checkpoints and logs if they don't exist\n",
    "# Example of training with the datasets\n",
    "import os\n",
    "import utils\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs('.logs', exist_ok=True)\n",
    "utils.clear_directory('.logs')\n",
    "\n",
    "history = model.fit(window.train, epochs=10,\n",
    "                        validation_data=window.val,\n",
    "                        callbacks=[\n",
    "                            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True, mode='min'),\n",
    "                            tf.keras.callbacks.ModelCheckpoint(filepath=f\"models/{ASSET}.st1.keras\", monitor='val_loss', save_best_only=True),\n",
    "                            tf.keras.callbacks.TensorBoard(log_dir='.logs'),\n",
    "                            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-5)\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(window.test, verbose=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAGE 2\n",
    "##### Predicting behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agregar_labels(df, window_size=6):\n",
    "    # Crear una columna para la decisi√≥n de compra o venta\n",
    "    df[f'label_{window_size}'] = (df['close'].shift(-window_size) > df['close']).astype(int)\n",
    "    df.dropna(inplace=True)  # Eliminar filas con NaN resultantes del shift\n",
    "    return df\n",
    "FORWARD_WINDOW = 6\n",
    "LABEL = f\"label_{FORWARD_WINDOW}\"\n",
    "# A√±adir la columna de labels al DataFrame\n",
    "df = agregar_labels(df, FORWARD_WINDOW)\n",
    "\n",
    "from feature_processor import FeatureProcessor\n",
    "\n",
    "df = FeatureProcessor.add_ta_features(df)\n",
    "df = FeatureProcessor.add_bar_features(df)\n",
    "df = FeatureProcessor.add_adj_features(df)\n",
    "df = FeatureProcessor.add_mv_avg_features(df)\n",
    "df = FeatureProcessor.add_time_features(df)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import class_weight\n",
    "\n",
    "# class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "# class_weights_dict = dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos que tu DataFrame se llama df\n",
    "# Reemplaza 'label_6' con el nombre de la columna que deseas analizar\n",
    "\n",
    "# label_column = LABEL\n",
    "label_column = SEQ_LABEL\n",
    "count_values = df[label_column].value_counts()\n",
    "\n",
    "print(f\"Conteo de valores en la columna {label_column}:\")\n",
    "print(count_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
    "\n",
    "n = len(df)\n",
    "train_df = df[0         :int(n*0.7)]\n",
    "val_df   = df[int(n*0.7):int(n*0.9)]\n",
    "test_df  = df[int(n*0.9):          ]\n",
    "\n",
    "num_features = df.shape[1]\n",
    "print(f\"Features count: {num_features}\")\n",
    "print(f\"Train count: {len(train_df)}\")\n",
    "print(f\"Validation count: {len(val_df)}\")\n",
    "print(f\"Test count: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_columns, features_columns,\n",
    "                 train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                  batch_size=32):\n",
    "        # Almacenar los datos crudos\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        self.column_indices = {name: i for i, name in enumerate(train_df.columns)}\n",
    "        \n",
    "        self.label_columns = label_columns     \n",
    "           \n",
    "        self.features_columns = features_columns\n",
    "        \n",
    "\n",
    "        # Calcular los par√°metros de la ventana\n",
    "        self.input_width = input_width\n",
    "\n",
    "        self.total_window_size = input_width \n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.normalize_inputs = False\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {min(self.input_indices)} - {max(self.input_indices)}',\n",
    "            f'Label column name(s): {self.label_columns}',\n",
    "            f'Feature column name(s): {self.features_columns}'\n",
    "        ])\n",
    "    \n",
    "    @tf.function\n",
    "    def normalize(self, inputs):\n",
    "        def normalize_sequence(sequence):\n",
    "            rolling_mean = tf.reduce_mean(sequence, axis=0, keepdims=True)\n",
    "            return (sequence - rolling_mean) / rolling_mean\n",
    "\n",
    "        return tf.map_fn(normalize_sequence, inputs, fn_output_signature=tf.float32)\n",
    "\n",
    "    def split_window(self, features):\n",
    "        data = features[:, self.input_slice, :]\n",
    "        inputs = tf.stack([data[:, :, self.column_indices[name]] for name in self.features_columns], axis=-1)\n",
    "        labels = tf.stack([data[:, -1, self.column_indices[name]] for name in self.label_columns], axis=-1)\n",
    "\n",
    "        if self.normalize_inputs:\n",
    "            inputs = self.normalize(tf.cast(inputs, tf.float32))  # Convertir inputs a float32\n",
    "\n",
    "        inputs.set_shape([None, self.input_width, len(self.features_columns)])\n",
    "        labels.set_shape([None, len(self.label_columns)])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = data.to_numpy()\n",
    "        ds = keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "        ds = ds.map(self.split_window)\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No se encontr√≥ un lote de ejemplo, as√≠ que obt√©n uno del dataset de entrenamiento\n",
    "            result = next(iter(self.train))\n",
    "            # Y gu√°rdalo en cach√© para la pr√≥xima vez\n",
    "            self._example = result\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns=[LABEL]\n",
    "# features_columns=['open', \"close\"]\n",
    "features_columns= [col for col in df.columns if col not in label_columns]\n",
    "num_labels = len(label_columns)\n",
    "num_features = len(features_columns)\n",
    "input_width = 120\n",
    "\n",
    "window = WindowGenerator( input_width=input_width, label_columns= label_columns , features_columns=features_columns, batch_size=2048)\n",
    "\n",
    "window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window.example[1][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_lstm_model(input_shape, num_labels):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.LSTM(64, return_sequences=True)(inputs)\n",
    "    x = attention_gate(x)\n",
    "    x = layers.LSTM(32, return_sequences=False)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(input_shape[0], activation='softmax')(x)\n",
    "    outputs = layers.Dense(num_labels, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def attention_gate(inputs):\n",
    "    attention = layers.Dense(1, activation='tanh')(inputs)\n",
    "    attention = layers.Flatten()(attention)\n",
    "    attention = layers.Activation('softmax')(attention)\n",
    "    attention = layers.RepeatVector(inputs.shape[-1])(attention)\n",
    "    attention = layers.Permute([2, 1])(attention)\n",
    "    attended_output = layers.Multiply()([inputs, attention])\n",
    "    return attended_output\n",
    "\n",
    "# Define input shape and number of labels\n",
    "input_shape = (input_width, num_features)  # Adjust based on your data\n",
    "\n",
    "# Create the model\n",
    "model = create_lstm_model(input_shape, num_labels)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    # keras.layers.Conv1D(32, 3, activation='relu'),\n",
    "    # keras.layers.BatchNormalization(),\n",
    "    keras.layers.LSTM(64, return_sequences=True),\n",
    "    # keras.layers.BatchNormalization(),\n",
    "    keras.layers.LSTM(32, return_sequences=False),\n",
    "    # keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(input_width, activation='softmax'),\n",
    "    keras.layers.Dense(num_labels, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window.normalize_inputs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for saving model checkpoints and logs if they don't exist\n",
    "# Example of training with the datasets\n",
    "import os\n",
    "import utils\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs('.logs', exist_ok=True)\n",
    "utils.clear_directory('.logs')\n",
    "\n",
    "history = model.fit(window.train, epochs=10,\n",
    "                        validation_data=window.val,\n",
    "                        callbacks=[\n",
    "                            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True, mode='min'),\n",
    "                            tf.keras.callbacks.ModelCheckpoint(filepath=f\"models/{ASSET}.lstm.keras\", monitor='val_loss', save_best_only=True),\n",
    "                            tf.keras.callbacks.TensorBoard(log_dir='.logs'),\n",
    "                            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-5)\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation data\n",
    "test_loss, test_accuracy = model.evaluate(window.test, verbose=1)\n",
    "\n",
    "# Convert accuracy to percentage\n",
    "test_accuracy_percentage = test_accuracy * 100\n",
    "\n",
    "print(f\"Validation Loss: {test_loss}\")\n",
    "print(f\"Validation Accuracy: {test_accuracy}\")\n",
    "print(f\"Validation Accuracy %: {test_accuracy_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(window.train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agregar_labels(df, window_size=6):\n",
    "    # Crear una columna para la decisi√≥n de compra o venta\n",
    "    df[f'label_{window_size}'] = (df['close'].shift(-window_size) > df['close']).astype(int)\n",
    "    df.dropna(inplace=True)  # Eliminar filas con NaN resultantes del shift\n",
    "    return df\n",
    "FORWARD_WINDOW = 6\n",
    "LABEL = f\"label_{FORWARD_WINDOW}\"\n",
    "# A√±adir la columna de labels al DataFrame\n",
    "df = agregar_labels(df, FORWARD_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "# scaler = StandardScaler()\n",
    "# scaler = Normalizer()\n",
    "scaler = MinMaxScaler()\n",
    "# df2 = df[LABEL]\n",
    "normalized_data = scaler.fit_transform(df)\n",
    "df2 = pd.DataFrame(data=normalized_data, columns=df.columns)\n",
    "# normalized_data = df.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = df.apply(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of clusters\n",
    "num_clusters = 3  # Adjust based on your specific needs\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(normalized_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the clusters using the first two features as an example\n",
    "plt.scatter(df.iloc[:, 3], df.iloc[:, 4], c=df['cluster'], cmap='viridis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('K-means Clustering of OHLC Data with Additional Features')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
